"""
DoceLang Lexer (Analisador L√©xico)
===================================

Implementa√ß√£o do analisador l√©xico para a linguagem DoceLang.
Converte c√≥digo-fonte em sequ√™ncia de tokens.

Autor: Projeto Compiladores 2025
Data: 15 de novembro de 2025
"""

import re
from enum import Enum
from typing import List, Optional
from dataclasses import dataclass


class TokenType(Enum):
    """Tipos de tokens da linguagem DoceLang"""
    
    # Palavras-chave
    RECIPE = 'RECIPE'
    INGREDIENTS = 'INGREDIENTS'
    PREPARATION = 'PREPARATION'
    ADD = 'ADD'
    MIX = 'MIX'
    HEAT = 'HEAT'
    WAIT = 'WAIT'
    SERVE = 'SERVE'
    REPEAT = 'REPEAT'
    TIMES = 'TIMES'
    
    # Delimitadores
    LBRACE = 'LBRACE'
    RBRACE = 'RBRACE'
    SEMICOLON = 'SEMICOLON'
    
    # Literais
    IDENTIFIER = 'IDENTIFIER'
    NUMBER = 'NUMBER'
    TIME = 'TIME'
    TEMPERATURE = 'TEMPERATURE'
    
    # Especiais
    EOF = 'EOF'
    
    def __str__(self):
        return self.value


@dataclass
class Token:
    """Representa um token identificado pelo lexer"""
    type: TokenType
    value: str
    line: int
    column: int
    
    def __repr__(self):
        return f"Token({self.type.value}, '{self.value}', {self.line}:{self.column})"


class LexicalError(Exception):
    """Exce√ß√£o para erros l√©xicos"""
    pass


class DoceLangLexer:
    """Analisador l√©xico para DoceLang"""
    
    # Palavras-chave da linguagem
    KEYWORDS = {
        'recipe': TokenType.RECIPE,
        'ingredients': TokenType.INGREDIENTS,
        'preparation': TokenType.PREPARATION,
        'add': TokenType.ADD,
        'mix': TokenType.MIX,
        'heat': TokenType.HEAT,
        'wait': TokenType.WAIT,
        'serve': TokenType.SERVE,
        'repeat': TokenType.REPEAT,
        'times': TokenType.TIMES,
    }
    
    # Padr√µes regex (ordem importa!)
    PATTERNS = [
        # Coment√°rios
        (r'//[^\n]*', None),  # Coment√°rio de linha
        (r'/\*.*?\*/', None),  # Coment√°rio de bloco
        
        # Tempo (antes de n√∫mero)
        (r'\d+(s|min|h)', TokenType.TIME),
        
        # Temperatura (antes de n√∫mero)
        (r'\d+(C|F)', TokenType.TEMPERATURE),
        
        # N√∫mero
        (r'\d+', TokenType.NUMBER),
        
        # Identificador (palavra-chave ou identificador)
        (r'[a-zA-Z][a-zA-Z0-9_]*', 'KEYWORD_OR_IDENTIFIER'),
        
        # Delimitadores
        (r'\{', TokenType.LBRACE),
        (r'\}', TokenType.RBRACE),
        (r';', TokenType.SEMICOLON),
        
        # Espa√ßos em branco (ignorar)
        (r'[ \t\n\r]+', None),
    ]
    
    def __init__(self, source_code: str):
        """
        Inicializa o lexer com c√≥digo-fonte
        
        Args:
            source_code: String contendo o c√≥digo DoceLang
        """
        self.source = source_code
        self.position = 0
        self.line = 1
        self.column = 1
        self.tokens: List[Token] = []
    
    def tokenize(self) -> List[Token]:
        """
        Realiza an√°lise l√©xica completa do c√≥digo
        
        Returns:
            Lista de tokens identificados
            
        Raises:
            LexicalError: Se encontrar caractere inv√°lido
        """
        while self.position < len(self.source):
            matched = False
            
            # Tentar casar cada padr√£o
            for pattern, token_type in self.PATTERNS:
                regex = re.compile(pattern, re.DOTALL)  # DOTALL permite . capturar \n
                match = regex.match(self.source, self.position)
                
                if match:
                    value = match.group(0)
                    
                    # Pular coment√°rios e espa√ßos
                    if token_type is None:
                        # Atualizar linha/coluna
                        for char in value:
                            if char == '\n':
                                self.line += 1
                                self.column = 1
                            else:
                                self.column += 1
                    
                    # Identificadores e palavras-chave
                    elif token_type == 'KEYWORD_OR_IDENTIFIER':
                        actual_type = self.KEYWORDS.get(value, TokenType.IDENTIFIER)
                        token = Token(actual_type, value, self.line, self.column)
                        self.tokens.append(token)
                        self.column += len(value)
                    
                    # Outros tokens
                    else:
                        token = Token(token_type, value, self.line, self.column)
                        self.tokens.append(token)
                        self.column += len(value)
                    
                    self.position = match.end()
                    matched = True
                    break
            
            if not matched:
                char = self.source[self.position]
                raise LexicalError(
                    f"Caractere inv√°lido '{char}' na linha {self.line}, "
                    f"coluna {self.column}"
                )
        
        # Adicionar EOF
        self.tokens.append(Token(TokenType.EOF, '', self.line, self.column))
        return self.tokens
    
    def get_tokens(self) -> List[Token]:
        """Retorna lista de tokens (tokeniza se necess√°rio)"""
        if not self.tokens:
            self.tokenize()
        return self.tokens


def main():
    """Fun√ß√£o principal para teste do lexer"""
    
    # C√≥digo de exemplo
    example_code = """
    /*
     * Exemplo: Brigadeiro
     */
    recipe Brigadeiro {
        ingredients {
            leite_condensado;
            chocolate_em_po;
            manteiga;
        }
        
        preparation {
            add leite_condensado;
            add chocolate_em_po;
            add manteiga;
            heat 180C;
            mix 15min;
            wait 2h;
            serve;
        }
    }
    """
    
    print("=" * 60)
    print("DOCELANG LEXER - AN√ÅLISE L√âXICA")
    print("=" * 60)
    print("\nC√≥digo de entrada:")
    print("-" * 60)
    print(example_code)
    print("-" * 60)
    
    try:
        lexer = DoceLangLexer(example_code)
        tokens = lexer.tokenize()
        
        print(f"\n‚úÖ An√°lise l√©xica conclu√≠da com sucesso!")
        print(f"üìä Total de tokens: {len(tokens)}\n")
        
        print("Tokens identificados:")
        print("-" * 60)
        
        for i, token in enumerate(tokens, 1):
            print(f"{i:3d}. {token}")
        
    except LexicalError as e:
        print(f"\n‚ùå ERRO L√âXICO: {e}")


if __name__ == '__main__':
    main()
