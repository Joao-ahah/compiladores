"""
DoceLang - Executor de Exemplos
================================

Script para executar o lexer em todos os arquivos de exemplo .doce

Autor: Projeto Compiladores 2025
"""

import os
import sys
from pathlib import Path

# Adicionar diretÃ³rio do lexer ao path
sys.path.insert(0, str(Path(__file__).parent / 'lexer'))

from lexer import DoceLangLexer, LexicalError
from tokens import print_tokens_table, validate_token_sequence


def process_file(filepath):
    """Processa um arquivo .doce"""
    print("\n" + "=" * 70)
    print(f"ğŸ“„ Arquivo: {filepath}")
    print("=" * 70)
    
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            code = f.read()
        
        print(f"\nğŸ“ CÃ³digo ({len(code)} caracteres, {len(code.splitlines())} linhas):")
        print("-" * 70)
        print(code)
        print("-" * 70)
        
        # Tokenizar
        lexer = DoceLangLexer(code)
        tokens = lexer.tokenize()
        
        print(f"\nâœ… TokenizaÃ§Ã£o concluÃ­da: {len(tokens)} tokens")
        
        # EstatÃ­sticas
        stats = {}
        for token in tokens:
            type_name = token.type.value
            stats[type_name] = stats.get(type_name, 0) + 1
        
        print("\nğŸ“Š EstatÃ­sticas de Tokens:")
        for token_type, count in sorted(stats.items(), key=lambda x: -x[1]):
            print(f"  {token_type:20s}: {count:3d}")
        
        # Tabela de tokens
        print("\nğŸ“‹ Tabela de Tokens:")
        print_tokens_table(tokens)
        
        # ValidaÃ§Ã£o
        is_valid, errors = validate_token_sequence(tokens)
        
        if is_valid:
            print("\nâœ… VALIDAÃ‡ÃƒO: SequÃªncia de tokens vÃ¡lida!")
        else:
            print("\nâš ï¸  VALIDAÃ‡ÃƒO: Problemas encontrados:")
            for error in errors:
                print(f"  - {error}")
        
        return True, len(tokens), stats
        
    except LexicalError as e:
        print(f"\nâŒ ERRO LÃ‰XICO: {e}")
        return False, 0, {}
        
    except FileNotFoundError:
        print(f"\nâŒ ERRO: Arquivo nÃ£o encontrado!")
        return False, 0, {}
        
    except Exception as e:
        print(f"\nâŒ ERRO: {e}")
        return False, 0, {}


def main():
    """FunÃ§Ã£o principal"""
    print("=" * 70)
    print("DOCELANG - ANÃLISE LÃ‰XICA DE EXEMPLOS")
    print("=" * 70)
    
    # DiretÃ³rio de exemplos
    examples_dir = Path(__file__).parent / 'examples'
    
    if not examples_dir.exists():
        print(f"\nâŒ DiretÃ³rio de exemplos nÃ£o encontrado: {examples_dir}")
        return
    
    # Listar arquivos .doce
    doce_files = list(examples_dir.glob('*.doce'))
    
    if not doce_files:
        print(f"\nâš ï¸  Nenhum arquivo .doce encontrado em {examples_dir}")
        return
    
    print(f"\nğŸ“‚ Encontrados {len(doce_files)} arquivo(s) .doce:")
    for f in doce_files:
        print(f"  - {f.name}")
    
    # Processar cada arquivo
    results = []
    for filepath in sorted(doce_files):
        success, token_count, stats = process_file(filepath)
        results.append({
            'file': filepath.name,
            'success': success,
            'tokens': token_count,
            'stats': stats
        })
    
    # Resumo final
    print("\n" + "=" * 70)
    print("RESUMO FINAL")
    print("=" * 70)
    
    successful = sum(1 for r in results if r['success'])
    failed = len(results) - successful
    
    print(f"\nâœ… Sucesso: {successful}/{len(results)}")
    print(f"âŒ Falhas:  {failed}/{len(results)}")
    
    if successful > 0:
        print("\nğŸ“Š EstatÃ­sticas Gerais:")
        total_tokens = sum(r['tokens'] for r in results if r['success'])
        print(f"  Total de tokens: {total_tokens}")
        print(f"  MÃ©dia por arquivo: {total_tokens / successful:.1f}")
        
        # Combinar estatÃ­sticas
        combined_stats = {}
        for r in results:
            if r['success']:
                for token_type, count in r['stats'].items():
                    combined_stats[token_type] = combined_stats.get(token_type, 0) + count
        
        print("\n  Tokens mais comuns:")
        for token_type, count in sorted(combined_stats.items(), key=lambda x: -x[1])[:10]:
            print(f"    {token_type:20s}: {count:3d}")
    
    print("\n" + "=" * 70)
    
    if failed == 0:
        print("ğŸ‰ TODOS OS EXEMPLOS FORAM PROCESSADOS COM SUCESSO! ğŸ‰")
    else:
        print(f"âš ï¸  {failed} exemplo(s) falharam")
    
    print("=" * 70)


if __name__ == '__main__':
    main()
